{"posts":[{"title":"Expectation","text":"Expectation of a Random Variable","link":"/2023/07/23/Statistics_Expectation/"},{"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension ì •ë¦¬","text":"ğŸ“ Facebookì—ì„œ ë°œí‘œí•œ ë…¼ë¬¸ğŸ“ ACL 2020 accepted ğŸ“„ë…¼ë¬¸ ì›ë³¸ ABSTRACTğŸ’¡ ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ë‚´ìš© pre-trainì„ ìœ„í•œ Seq2Seq model, BART ì œì•ˆ BART â†’ standard Transformer-based Architecture corrupted textë¥¼ original textì— mapping denoising autoencoder INTRODUCTIONğŸ’¡ ì´ì „ ì—°êµ¬ self-supervised methodëŠ” ë‹¤ì–‘í•œ NLP taskì—ì„œ ì£¼ëª©í• ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ ê·¸ì¤‘ ë‹¨ì–´ì˜ subsetì´ random masked textë¥¼ reconstruct í•˜ë„ë¡ í•™ìŠµëœ denoising autoencoder(DAE) ë°©ë²•ì´ ì„±ê³µì  â†’ MLMì˜ ë³€í˜• maksed tokenì˜ distribution, ì˜ˆì¸¡ ìˆœì„œ, ëŒ€ì²´ ê°€ëŠ¥í•œ context ë“±ì„ ê°œì„ í•´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ self-supervised method ì •ì˜ : unlabeled dataë¡œ ì¢‹ì€ representationì„ ì–»ê³ ìí•˜ëŠ” í•™ìŠµ ë°©ì‹ â†’ label(y) ì—†ì´ input(x) ë‚´ì—ì„œ targetìœ¼ë¡œ ì“°ì¼ë§Œ í•œ ê²ƒì„ ì •í•´ì„œ ì¦‰ selfë¡œ taskë¥¼ ì •í•´ì„œ supervision ë°©ì‹ìœ¼ë¡œ ëª¨ë¸ì„ í•™ìŠµ DAE ë§ê·¸ëŒ€ë¡œ inputì— noiseë¥¼ ì¶”ê°€í–ˆìŒì—ë„ ë””ì½”ë”ì˜ outputì€ noiseë¥¼ ì¶”ê°€í•˜ì§€ ì•Šì€ ì›ë³¸ì„ ë³µì›í•˜ë„ë¡ í•˜ìëŠ” ì•„ì´ë””ì–´ ğŸ’¡ ì œì•ˆ ë°°ê²½ ë‹¹ì‹œ ìµœê·¼ ì—°êµ¬ ë°©ë²•ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ span predictionì´ë‚˜ generationê³¼ ê°™ì€ íŠ¹ì •í•œ ìœ í˜•ì˜ end taskì— ì´ˆì ì„ ë§ì¶¤ â†’ ì ìš© ê°€ëŠ¥ì„±ì´ ì œí•œì  ex) BERTëŠ” generation task ì ìš© ë¶ˆê°€ëŠ¥, GPTëŠ” bidirectional context ì •ë³´ ë°˜ì˜ ë¶ˆê°€ëŠ¥ ğŸ’¡ BART : standard Transformer-based architectureë¥¼ ê°€ì§€ë©°, Bidirectionalê³¼ Auto-Regressive Transformerë¥¼ í•©ì¹œ model â‡’ BERTì˜ encoder + GPTì˜ decoder ![(a) BERT: random tokenì´ maskë¡œ ëŒ€ì²´ë˜ê³  ë¬¸ì„œê°€ bidirectionlë¡œ encodingë¨ .ëˆ„ë½ëœ tokenì€ ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡ë˜ë¯€ë¡œ BERTëŠ” generationì— ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ x(b) GPT: tokenì€ auto-regressiveí•˜ê²Œ ì˜ˆì¸¡ë˜ë¯€ë¡œ GPTë¥¼ generationì— ì‚¬ìš© ê°€ëŠ¥. But ë‹¨ì–´ëŠ” ì™¼ìª½ contextì—ë§Œ ì¡°ê±´ì´ ì ìš©ë˜ë¯€ë¡œ bidirectional ìƒí˜¸ ì‘ìš©ì„ í•™ìŠµí•  ìˆ˜ x(c) BART: encoderì— ëŒ€í•œ inputì´ decoderì˜ outputê³¼ ì¼ì¹˜í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ arbitary noise transformationì´ ê°€ëŠ¥. ì—¬ê¸°ì„œëŠ” textì˜ spanì„ ë¡œ ëŒ€ì²´í•˜ì—¬ ë¬¸ì„œê°€ ì†ìƒë¨ â†’ ì†ìƒëœ ë¬¸ì„œ(ì™¼ìª½)ëŠ” bidirectional modelë¡œ encodingí•œ ë‹¤ìŒ autoregressive decoderë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ë¬¸ì„œ(ì˜¤ë¥¸ìª½)ì˜ ê°€ëŠ¥ì„±ì„ ê³„ì‚°. fine-tuningì„ ìœ„í•´ ì†ìƒë˜ì§€ ì•Šì€ ë¬¸ì„œë¥¼ encoderì™€ decoderì— ëª¨ë‘ ì…ë ¥í•œ í›„ decoderì˜ final hidden stateì˜ represenationì„ ì‚¬ìš©.](BART%20Denoising%20Sequence-to-Sequence%20Pre-training%20f%2071d1b4e36ff54c80a818e8e3080efeb3/Untitled.png) (a) BERT: random tokenì´ maskë¡œ ëŒ€ì²´ë˜ê³  ë¬¸ì„œê°€ bidirectionlë¡œ encodingë¨ .ëˆ„ë½ëœ tokenì€ ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡ë˜ë¯€ë¡œ BERTëŠ” generationì— ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ x(b) GPT: tokenì€ auto-regressiveí•˜ê²Œ ì˜ˆì¸¡ë˜ë¯€ë¡œ GPTë¥¼ generationì— ì‚¬ìš© ê°€ëŠ¥. But ë‹¨ì–´ëŠ” ì™¼ìª½ contextì—ë§Œ ì¡°ê±´ì´ ì ìš©ë˜ë¯€ë¡œ bidirectional ìƒí˜¸ ì‘ìš©ì„ í•™ìŠµí•  ìˆ˜ x(c) BART: encoderì— ëŒ€í•œ inputì´ decoderì˜ outputê³¼ ì¼ì¹˜í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ arbitary noise transformationì´ ê°€ëŠ¥. ì—¬ê¸°ì„œëŠ” textì˜ spanì„ ë¡œ ëŒ€ì²´í•˜ì—¬ ë¬¸ì„œê°€ ì†ìƒë¨ â†’ ì†ìƒëœ ë¬¸ì„œ(ì™¼ìª½)ëŠ” bidirectional modelë¡œ encodingí•œ ë‹¤ìŒ autoregressive decoderë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ë¬¸ì„œ(ì˜¤ë¥¸ìª½)ì˜ ê°€ëŠ¥ì„±ì„ ê³„ì‚°. fine-tuningì„ ìœ„í•´ ì†ìƒë˜ì§€ ì•Šì€ ë¬¸ì„œë¥¼ encoderì™€ decoderì— ëª¨ë‘ ì…ë ¥í•œ í›„ decoderì˜ final hidden stateì˜ represenationì„ ì‚¬ìš©. 2-step of Pre-train ì„ì˜ì˜ noising functionìœ¼ë¡œ textë¥¼ ì†ìƒì‹œí‚´ original textë¥¼ reconstructí•˜ë„ë¡ Seq2Seqë¥¼ í•™ìŠµ ì¥ì  noising flexibility â†’ original textì— ê¸¸ì´ ë³€ê²½ì„ í¬í•¨í•œ ì„ì˜ì˜ transformation ì ìš© ê°€ëŠ¥ ì—¬ëŸ¬ noising approach ì‹¤í—˜ â†’ (1) original textì˜ ìˆœì„œë¥¼ ëœë¤ìœ¼ë¡œ ì„ëŠ” ë°©ì‹ + (2) ì„ì˜ì˜ ê¸¸ì´ ë²”ìœ„(0 í¬í•¨)ì˜ textë¥¼ single mask tokenìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ìƒˆë¡œìš´ in-filling ë°©ì‹ â†’ SOTAë¥¼ ì´ë£¨ëŠ” approach ë°œê²¬ modelì´ ì „ì²´ ë¬¸ì¥ ê¸¸ì´ì— ëŒ€í•´ ë” ë§ì´ ì¶”ë¡ í•˜ê³ , inputì— ë” ê¸´ ë²”ìœ„ì˜ transformationì„ ìˆ˜í–‰í•˜ê²Œ í•¨ â†’ MLMê³¼ NSPë¥¼ generalize text generationì„ ìœ„í•œ fine-tuningì— íš¨ê³¼ì , comprehension taskì—ì„œë„ ì˜ ì‘ë™ fine-tuningì— ëŒ€í•œ ìƒˆë¡œìš´ ì‚¬ê³  ë°©ì‹ í™•ì¥ BART ìœ„ì— ì¶”ê°€ì ì¸ transformer layerë¥¼ ìŒ“ìŒ â†’ ì™¸êµ­ì–´ë¥¼ noisingëœ ì˜ì–´ë¡œ mappingí•˜ê³  ì´ë¥¼ ë‹¤ì‹œ denoisingí•˜ëŠ”ë° ì‚¬ìš©í•¨ìœ¼ë¡œì¨ BARTê°€ ê°•ë ¥í•œ ë²ˆì—­ëª¨ë¸ë¡œ ë™ì‘í•˜ê²Œ í•¨","link":"/2023/07/31/NLP_BART_Paper/"},{"title":"Sound Event Detection: A Tutorial ë…¼ë¬¸ ì •ë¦¬","text":"SOUND EVENTS INT OUR EVERYDAY ENVIRONMENT","link":"/2023/07/26/DSP_SED_Paper/"}],"tags":[{"name":"statistics","slug":"statistics","link":"/tags/statistics/"},{"name":"PLM","slug":"PLM","link":"/tags/PLM/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"Paper Review","slug":"Paper-Review","link":"/tags/Paper-Review/"},{"name":"SED","slug":"SED","link":"/tags/SED/"},{"name":"DSP","slug":"DSP","link":"/tags/DSP/"}],"categories":[{"name":"Paper Review","slug":"Paper-Review","link":"/categories/Paper-Review/"},{"name":"NLP","slug":"Paper-Review/NLP","link":"/categories/Paper-Review/NLP/"},{"name":"DSP","slug":"Paper-Review/DSP","link":"/categories/Paper-Review/DSP/"}],"pages":[]}