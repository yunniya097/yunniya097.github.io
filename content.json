{"posts":[{"title":"Expectation","text":"Expectation of a Random Variable","link":"/2023/07/23/Statistics_Expectation/"},{"title":"BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension 정리","text":"📍 Facebook에서 발표한 논문📍 ACL 2020 accepted 📄논문 원본 ABSTRACT💡 본 논문에서 제안하는 내용 pre-train을 위한 Seq2Seq model, BART 제안 BART → standard Transformer-based Architecture corrupted text를 original text에 mapping denoising autoencoder INTRODUCTION💡 이전 연구 self-supervised method는 다양한 NLP task에서 주목할만한 성능을 보여줌 그중 단어의 subset이 random masked text를 reconstruct 하도록 학습된 denoising autoencoder(DAE) 방법이 성공적 → MLM의 변형 maksed token의 distribution, 예측 순서, 대체 가능한 context 등을 개선해 더 나은 결과를 보여줌 self-supervised method 정의 : unlabeled data로 좋은 representation을 얻고자하는 학습 방식 → label(y) 없이 input(x) 내에서 target으로 쓰일만 한 것을 정해서 즉 self로 task를 정해서 supervision 방식으로 모델을 학습 DAE 말그대로 input에 noise를 추가했음에도 디코더의 output은 noise를 추가하지 않은 원본을 복원하도록 하자는 아이디어 💡 제안 배경 당시 최근 연구 방법들은 일반적으로 span prediction이나 generation과 같은 특정한 유형의 end task에 초점을 맞춤 → 적용 가능성이 제한적 ex) BERT는 generation task 적용 불가능, GPT는 bidirectional context 정보 반영 불가능 💡 BART : standard Transformer-based architecture를 가지며, Bidirectional과 Auto-Regressive Transformer를 합친 model ⇒ BERT의 encoder + GPT의 decoder ![(a) BERT: random token이 mask로 대체되고 문서가 bidirectionl로 encoding됨 .누락된 token은 독립적으로 예측되므로 BERT는 generation에 쉽게 사용할 수 x(b) GPT: token은 auto-regressive하게 예측되므로 GPT를 generation에 사용 가능. But 단어는 왼쪽 context에만 조건이 적용되므로 bidirectional 상호 작용을 학습할 수 x(c) BART: encoder에 대한 input이 decoder의 output과 일치할 필요가 없으므로 arbitary noise transformation이 가능. 여기서는 text의 span을 로 대체하여 문서가 손상됨 → 손상된 문서(왼쪽)는 bidirectional model로 encoding한 다음 autoregressive decoder를 사용하여 원본 문서(오른쪽)의 가능성을 계산. fine-tuning을 위해 손상되지 않은 문서를 encoder와 decoder에 모두 입력한 후 decoder의 final hidden state의 represenation을 사용.](BART%20Denoising%20Sequence-to-Sequence%20Pre-training%20f%2071d1b4e36ff54c80a818e8e3080efeb3/Untitled.png) (a) BERT: random token이 mask로 대체되고 문서가 bidirectionl로 encoding됨 .누락된 token은 독립적으로 예측되므로 BERT는 generation에 쉽게 사용할 수 x(b) GPT: token은 auto-regressive하게 예측되므로 GPT를 generation에 사용 가능. But 단어는 왼쪽 context에만 조건이 적용되므로 bidirectional 상호 작용을 학습할 수 x(c) BART: encoder에 대한 input이 decoder의 output과 일치할 필요가 없으므로 arbitary noise transformation이 가능. 여기서는 text의 span을 로 대체하여 문서가 손상됨 → 손상된 문서(왼쪽)는 bidirectional model로 encoding한 다음 autoregressive decoder를 사용하여 원본 문서(오른쪽)의 가능성을 계산. fine-tuning을 위해 손상되지 않은 문서를 encoder와 decoder에 모두 입력한 후 decoder의 final hidden state의 represenation을 사용. 2-step of Pre-train 임의의 noising function으로 text를 손상시킴 original text를 reconstruct하도록 Seq2Seq를 학습 장점 noising flexibility → original text에 길이 변경을 포함한 임의의 transformation 적용 가능 여러 noising approach 실험 → (1) original text의 순서를 랜덤으로 섞는 방식 + (2) 임의의 길이 범위(0 포함)의 text를 single mask token으로 대체하는 새로운 in-filling 방식 → SOTA를 이루는 approach 발견 model이 전체 문장 길이에 대해 더 많이 추론하고, input에 더 긴 범위의 transformation을 수행하게 함 → MLM과 NSP를 generalize text generation을 위한 fine-tuning에 효과적, comprehension task에서도 잘 작동 fine-tuning에 대한 새로운 사고 방식 확장 BART 위에 추가적인 transformer layer를 쌓음 → 외국어를 noising된 영어로 mapping하고 이를 다시 denoising하는데 사용함으로써 BART가 강력한 번역모델로 동작하게 함","link":"/2023/07/31/NLP_BART_Paper/"},{"title":"Sound Event Detection: A Tutorial 논문 정리","text":"SOUND EVENTS INT OUR EVERYDAY ENVIRONMENT","link":"/2023/07/26/DSP_SED_Paper/"}],"tags":[{"name":"statistics","slug":"statistics","link":"/tags/statistics/"},{"name":"PLM","slug":"PLM","link":"/tags/PLM/"},{"name":"NLP","slug":"NLP","link":"/tags/NLP/"},{"name":"Paper Review","slug":"Paper-Review","link":"/tags/Paper-Review/"},{"name":"SED","slug":"SED","link":"/tags/SED/"},{"name":"DSP","slug":"DSP","link":"/tags/DSP/"}],"categories":[{"name":"Paper Review","slug":"Paper-Review","link":"/categories/Paper-Review/"},{"name":"NLP","slug":"Paper-Review/NLP","link":"/categories/Paper-Review/NLP/"},{"name":"DSP","slug":"Paper-Review/DSP","link":"/categories/Paper-Review/DSP/"}],"pages":[]}