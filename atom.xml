<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>YUNNI-CODING</title>
  
  <subtitle>Tech-Blog</subtitle>
  <link href="http://yunniya097.github.io/atom.xml" rel="self"/>
  
  <link href="http://yunniya097.github.io/"/>
  <updated>2023-08-03T08:15:43.704Z</updated>
  <id>http://yunniya097.github.io/</id>
  
  <author>
    <name>Yunniya</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension ì •ë¦¬</title>
    <link href="http://yunniya097.github.io/2023/07/31/NLP_BART_Paper/"/>
    <id>http://yunniya097.github.io/2023/07/31/NLP_BART_Paper/</id>
    <published>2023-07-31T06:26:03.000Z</published>
    <updated>2023-08-03T08:15:43.704Z</updated>
    
    <content type="html"><![CDATA[<p>ğŸ“Â Facebookì—ì„œ ë°œí‘œí•œ ë…¼ë¬¸<br>ğŸ“Â ACL 2020 accepted</p><p>ğŸ“„<a href="https://arxiv.org/pdf/1910.13461.pdf">ë…¼ë¬¸ ì›ë³¸</a></p><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a><strong>ABSTRACT</strong></h2><p>ğŸ’¡Â ë³¸ ë…¼ë¬¸ì—ì„œ ì œì•ˆí•˜ëŠ” ë‚´ìš©</p><ul><li>pre-trainì„ ìœ„í•œ Seq2Seq model, BART ì œì•ˆ</li><li>BART â†’ standard Transformer-based  Architecture<ul><li>corrupted textë¥¼ original textì— mapping denoising autoencoder</li></ul></li></ul><hr><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a><strong>INTRODUCTION</strong></h2><p>ğŸ’¡Â <strong>ì´ì „ ì—°êµ¬</strong></p><p>self-supervised methodëŠ” ë‹¤ì–‘í•œ NLP taskì—ì„œ ì£¼ëª©í• ë§Œí•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ</p><ul><li><p>ê·¸ì¤‘ ë‹¨ì–´ì˜ subsetì´ random masked textë¥¼ reconstruct í•˜ë„ë¡ í•™ìŠµëœ denoising autoencoder(DAE) ë°©ë²•ì´ ì„±ê³µì  â†’ MLMì˜ ë³€í˜•</p></li><li><p>maksed tokenì˜ distribution, ì˜ˆì¸¡ ìˆœì„œ, ëŒ€ì²´ ê°€ëŠ¥í•œ context ë“±ì„ ê°œì„ í•´ ë” ë‚˜ì€ ê²°ê³¼ë¥¼ ë³´ì—¬ì¤Œ</p></li><li><p>self-supervised method ì •ì˜</p><p>  : unlabeled dataë¡œ ì¢‹ì€ representationì„ ì–»ê³ ìí•˜ëŠ” í•™ìŠµ ë°©ì‹</p><p>  â†’ <strong>label(y) ì—†ì´ input(x) ë‚´ì—ì„œ targetìœ¼ë¡œ ì“°ì¼ë§Œ í•œ ê²ƒì„ ì •í•´ì„œ ì¦‰ selfë¡œ taskë¥¼ ì •í•´ì„œ supervision ë°©ì‹ìœ¼ë¡œÂ  ëª¨ë¸ì„ í•™ìŠµ</strong></p></li><li><p>DAE</p><p>  ë§ê·¸ëŒ€ë¡œ inputì— noiseë¥¼ ì¶”ê°€í–ˆìŒì—ë„ ë””ì½”ë”ì˜ outputì€ noiseë¥¼ ì¶”ê°€í•˜ì§€ ì•Šì€ ì›ë³¸ì„ ë³µì›í•˜ë„ë¡ í•˜ìëŠ” ì•„ì´ë””ì–´</p></li></ul><p>ğŸ’¡Â <strong>ì œì•ˆ ë°°ê²½</strong></p><p>ë‹¹ì‹œ ìµœê·¼ ì—°êµ¬ ë°©ë²•ë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ span predictionì´ë‚˜ generationê³¼ ê°™ì€ íŠ¹ì •í•œ ìœ í˜•ì˜ end taskì— ì´ˆì ì„ ë§ì¶¤ â†’ ì ìš© ê°€ëŠ¥ì„±ì´ ì œí•œì </p><p>ex) BERTëŠ” generation task ì ìš© ë¶ˆê°€ëŠ¥, GPTëŠ” bidirectional context ì •ë³´ ë°˜ì˜ ë¶ˆê°€ëŠ¥</p><p>ğŸ’¡Â <strong>BART</strong></p><p>: standard Transformer-based architectureë¥¼ ê°€ì§€ë©°, <strong>B</strong>idirectionalê³¼ <strong>A</strong>uto-<strong>R</strong>egressive <strong>T</strong>ransformerë¥¼ í•©ì¹œ model</p><p>â‡’ BERTì˜ encoder + GPTì˜ decoder</p><h2 id="a-BERT-random-tokenì´-maskë¡œ-ëŒ€ì²´ë˜ê³ -ë¬¸ì„œê°€-bidirectionlë¡œ-encodingë¨-ëˆ„ë½ëœ-tokenì€-ë…ë¦½ì ìœ¼ë¡œ-ì˜ˆì¸¡ë˜ë¯€ë¡œ-BERTëŠ”-generationì—-ì‰½ê²Œ-ì‚¬ìš©í• -ìˆ˜-x"><a href="#a-BERT-random-tokenì´-maskë¡œ-ëŒ€ì²´ë˜ê³ -ë¬¸ì„œê°€-bidirectionlë¡œ-encodingë¨-ëˆ„ë½ëœ-tokenì€-ë…ë¦½ì ìœ¼ë¡œ-ì˜ˆì¸¡ë˜ë¯€ë¡œ-BERTëŠ”-generationì—-ì‰½ê²Œ-ì‚¬ìš©í• -ìˆ˜-x" class="headerlink" title="![(a) BERT: random tokenì´ maskë¡œ ëŒ€ì²´ë˜ê³  ë¬¸ì„œê°€ bidirectionlë¡œ encodingë¨ .ëˆ„ë½ëœ tokenì€ ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡ë˜ë¯€ë¡œ BERTëŠ” generationì— ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ x"></a>![(a) BERT: random tokenì´ maskë¡œ ëŒ€ì²´ë˜ê³  ë¬¸ì„œê°€ bidirectionlë¡œ encodingë¨ .ëˆ„ë½ëœ tokenì€ ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡ë˜ë¯€ë¡œ BERTëŠ” generationì— ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ x</h2><h2 id="b-GPT-tokenì€-auto-regressiveí•˜ê²Œ-ì˜ˆì¸¡ë˜ë¯€ë¡œ-GPTë¥¼-generationì—-ì‚¬ìš©-ê°€ëŠ¥-But-ë‹¨ì–´ëŠ”-ì™¼ìª½-contextì—ë§Œ-ì¡°ê±´ì´-ì ìš©ë˜ë¯€ë¡œ-bidirectional-ìƒí˜¸-ì‘ìš©ì„-í•™ìŠµí• -ìˆ˜-x"><a href="#b-GPT-tokenì€-auto-regressiveí•˜ê²Œ-ì˜ˆì¸¡ë˜ë¯€ë¡œ-GPTë¥¼-generationì—-ì‚¬ìš©-ê°€ëŠ¥-But-ë‹¨ì–´ëŠ”-ì™¼ìª½-contextì—ë§Œ-ì¡°ê±´ì´-ì ìš©ë˜ë¯€ë¡œ-bidirectional-ìƒí˜¸-ì‘ìš©ì„-í•™ìŠµí• -ìˆ˜-x" class="headerlink" title="(b) GPT: tokenì€ auto-regressiveí•˜ê²Œ ì˜ˆì¸¡ë˜ë¯€ë¡œ GPTë¥¼ generationì— ì‚¬ìš© ê°€ëŠ¥. But ë‹¨ì–´ëŠ” ì™¼ìª½ contextì—ë§Œ ì¡°ê±´ì´ ì ìš©ë˜ë¯€ë¡œ bidirectional ìƒí˜¸ ì‘ìš©ì„ í•™ìŠµí•  ìˆ˜ x"></a>(b) GPT: tokenì€ auto-regressiveí•˜ê²Œ ì˜ˆì¸¡ë˜ë¯€ë¡œ GPTë¥¼ generationì— ì‚¬ìš© ê°€ëŠ¥. But ë‹¨ì–´ëŠ” ì™¼ìª½ contextì—ë§Œ ì¡°ê±´ì´ ì ìš©ë˜ë¯€ë¡œ bidirectional ìƒí˜¸ ì‘ìš©ì„ í•™ìŠµí•  ìˆ˜ x</h2><p>(c) BART: encoderì— ëŒ€í•œ inputì´ decoderì˜ outputê³¼ ì¼ì¹˜í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ arbitary noise transformationì´ ê°€ëŠ¥. ì—¬ê¸°ì„œëŠ” textì˜ spanì„ <MASK>ë¡œ ëŒ€ì²´í•˜ì—¬ ë¬¸ì„œê°€ ì†ìƒë¨ â†’ ì†ìƒëœ ë¬¸ì„œ(ì™¼ìª½)ëŠ” bidirectional modelë¡œ encodingí•œ ë‹¤ìŒ autoregressive decoderë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ë¬¸ì„œ(ì˜¤ë¥¸ìª½)ì˜ ê°€ëŠ¥ì„±ì„ ê³„ì‚°. fine-tuningì„ ìœ„í•´ ì†ìƒë˜ì§€ ì•Šì€ ë¬¸ì„œë¥¼ encoderì™€ decoderì— ëª¨ë‘ ì…ë ¥í•œ í›„ decoderì˜ final hidden stateì˜ represenationì„ ì‚¬ìš©.](BART%20Denoising%20Sequence-to-Sequence%20Pre-training%20f%2071d1b4e36ff54c80a818e8e3080efeb3&#x2F;Untitled.png)</p><h2 id="a-BERT-random-tokenì´-maskë¡œ-ëŒ€ì²´ë˜ê³ -ë¬¸ì„œê°€-bidirectionlë¡œ-encodingë¨-ëˆ„ë½ëœ-tokenì€-ë…ë¦½ì ìœ¼ë¡œ-ì˜ˆì¸¡ë˜ë¯€ë¡œ-BERTëŠ”-generationì—-ì‰½ê²Œ-ì‚¬ìš©í• -ìˆ˜-x-1"><a href="#a-BERT-random-tokenì´-maskë¡œ-ëŒ€ì²´ë˜ê³ -ë¬¸ì„œê°€-bidirectionlë¡œ-encodingë¨-ëˆ„ë½ëœ-tokenì€-ë…ë¦½ì ìœ¼ë¡œ-ì˜ˆì¸¡ë˜ë¯€ë¡œ-BERTëŠ”-generationì—-ì‰½ê²Œ-ì‚¬ìš©í• -ìˆ˜-x-1" class="headerlink" title="(a) BERT: random tokenì´ maskë¡œ ëŒ€ì²´ë˜ê³  ë¬¸ì„œê°€ bidirectionlë¡œ encodingë¨ .ëˆ„ë½ëœ tokenì€ ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡ë˜ë¯€ë¡œ BERTëŠ” generationì— ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ x"></a>(a) BERT: random tokenì´ maskë¡œ ëŒ€ì²´ë˜ê³  ë¬¸ì„œê°€ bidirectionlë¡œ encodingë¨ .ëˆ„ë½ëœ tokenì€ ë…ë¦½ì ìœ¼ë¡œ ì˜ˆì¸¡ë˜ë¯€ë¡œ BERTëŠ” generationì— ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ x</h2><h2 id="b-GPT-tokenì€-auto-regressiveí•˜ê²Œ-ì˜ˆì¸¡ë˜ë¯€ë¡œ-GPTë¥¼-generationì—-ì‚¬ìš©-ê°€ëŠ¥-But-ë‹¨ì–´ëŠ”-ì™¼ìª½-contextì—ë§Œ-ì¡°ê±´ì´-ì ìš©ë˜ë¯€ë¡œ-bidirectional-ìƒí˜¸-ì‘ìš©ì„-í•™ìŠµí• -ìˆ˜-x-1"><a href="#b-GPT-tokenì€-auto-regressiveí•˜ê²Œ-ì˜ˆì¸¡ë˜ë¯€ë¡œ-GPTë¥¼-generationì—-ì‚¬ìš©-ê°€ëŠ¥-But-ë‹¨ì–´ëŠ”-ì™¼ìª½-contextì—ë§Œ-ì¡°ê±´ì´-ì ìš©ë˜ë¯€ë¡œ-bidirectional-ìƒí˜¸-ì‘ìš©ì„-í•™ìŠµí• -ìˆ˜-x-1" class="headerlink" title="(b) GPT: tokenì€ auto-regressiveí•˜ê²Œ ì˜ˆì¸¡ë˜ë¯€ë¡œ GPTë¥¼ generationì— ì‚¬ìš© ê°€ëŠ¥. But ë‹¨ì–´ëŠ” ì™¼ìª½ contextì—ë§Œ ì¡°ê±´ì´ ì ìš©ë˜ë¯€ë¡œ bidirectional ìƒí˜¸ ì‘ìš©ì„ í•™ìŠµí•  ìˆ˜ x"></a>(b) GPT: tokenì€ auto-regressiveí•˜ê²Œ ì˜ˆì¸¡ë˜ë¯€ë¡œ GPTë¥¼ generationì— ì‚¬ìš© ê°€ëŠ¥. But ë‹¨ì–´ëŠ” ì™¼ìª½ contextì—ë§Œ ì¡°ê±´ì´ ì ìš©ë˜ë¯€ë¡œ bidirectional ìƒí˜¸ ì‘ìš©ì„ í•™ìŠµí•  ìˆ˜ x</h2><p>(c) BART: encoderì— ëŒ€í•œ inputì´ decoderì˜ outputê³¼ ì¼ì¹˜í•  í•„ìš”ê°€ ì—†ìœ¼ë¯€ë¡œ arbitary noise transformationì´ ê°€ëŠ¥. ì—¬ê¸°ì„œëŠ” textì˜ spanì„ <MASK>ë¡œ ëŒ€ì²´í•˜ì—¬ ë¬¸ì„œê°€ ì†ìƒë¨ â†’ ì†ìƒëœ ë¬¸ì„œ(ì™¼ìª½)ëŠ” bidirectional modelë¡œ encodingí•œ ë‹¤ìŒ autoregressive decoderë¥¼ ì‚¬ìš©í•˜ì—¬ ì›ë³¸ ë¬¸ì„œ(ì˜¤ë¥¸ìª½)ì˜ ê°€ëŠ¥ì„±ì„ ê³„ì‚°. fine-tuningì„ ìœ„í•´ ì†ìƒë˜ì§€ ì•Šì€ ë¬¸ì„œë¥¼ encoderì™€ decoderì— ëª¨ë‘ ì…ë ¥í•œ í›„ decoderì˜ final hidden stateì˜ represenationì„ ì‚¬ìš©.</p><p><strong>2-step of Pre-train</strong></p><ol><li><p>ì„ì˜ì˜ noising functionìœ¼ë¡œ textë¥¼ ì†ìƒì‹œí‚´</p></li><li><p>original textë¥¼ reconstructí•˜ë„ë¡ Seq2Seqë¥¼ í•™ìŠµ</p></li></ol><p><strong>ì¥ì </strong></p><ul><li>noising flexibility â†’ original textì— ê¸¸ì´ ë³€ê²½ì„ í¬í•¨í•œ ì„ì˜ì˜ transformation ì ìš© ê°€ëŠ¥<ul><li><p>ì—¬ëŸ¬ noising approach ì‹¤í—˜</p><p>  â†’  <strong>(1) original textì˜ ìˆœì„œë¥¼ ëœë¤ìœ¼ë¡œ ì„ëŠ” ë°©ì‹</strong> + <strong>(2) ì„ì˜ì˜ ê¸¸ì´ ë²”ìœ„(0 í¬í•¨)ì˜ textë¥¼ single mask tokenìœ¼ë¡œ ëŒ€ì²´í•˜ëŠ” ìƒˆë¡œìš´ in-filling ë°©ì‹</strong></p><p>  â†’ SOTAë¥¼ ì´ë£¨ëŠ” approach ë°œê²¬</p></li><li><p>modelì´ ì „ì²´ ë¬¸ì¥ ê¸¸ì´ì— ëŒ€í•´ ë” ë§ì´ ì¶”ë¡ í•˜ê³ , inputì— ë” ê¸´ ë²”ìœ„ì˜ transformationì„ ìˆ˜í–‰í•˜ê²Œ í•¨ â†’ MLMê³¼ NSPë¥¼ generalize</p></li></ul></li><li>text generationì„ ìœ„í•œ fine-tuningì— íš¨ê³¼ì , comprehension taskì—ì„œë„ ì˜ ì‘ë™</li><li>fine-tuningì— ëŒ€í•œ ìƒˆë¡œìš´ ì‚¬ê³  ë°©ì‹ í™•ì¥<ul><li><p>BART ìœ„ì— ì¶”ê°€ì ì¸ transformer layerë¥¼ ìŒ“ìŒ</p><p>  â†’ ì™¸êµ­ì–´ë¥¼ noisingëœ ì˜ì–´ë¡œ mappingí•˜ê³  ì´ë¥¼ ë‹¤ì‹œ denoisingí•˜ëŠ”ë° ì‚¬ìš©í•¨ìœ¼ë¡œì¨ BARTê°€ ê°•ë ¥í•œ ë²ˆì—­ëª¨ë¸ë¡œ ë™ì‘í•˜ê²Œ í•¨</p></li></ul></li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;ğŸ“Â Facebookì—ì„œ ë°œí‘œí•œ ë…¼ë¬¸&lt;br&gt;ğŸ“Â ACL 2020 accepted&lt;/p&gt;
&lt;p&gt;ğŸ“„&lt;a href=&quot;https://arxiv.org/pdf/1910.13461.pdf&quot;&gt;ë…¼ë¬¸ ì›ë³¸&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;ABSTRACT&quot;&gt;&lt;a h</summary>
      
    
    
    
    <category term="Paper Review" scheme="http://yunniya097.github.io/categories/Paper-Review/"/>
    
    <category term="NLP" scheme="http://yunniya097.github.io/categories/Paper-Review/NLP/"/>
    
    
    <category term="PLM" scheme="http://yunniya097.github.io/tags/PLM/"/>
    
    <category term="NLP" scheme="http://yunniya097.github.io/tags/NLP/"/>
    
    <category term="Paper Review" scheme="http://yunniya097.github.io/tags/Paper-Review/"/>
    
  </entry>
  
  <entry>
    <title>Sound Event Detection: A Tutorial ë…¼ë¬¸ ì •ë¦¬</title>
    <link href="http://yunniya097.github.io/2023/07/26/DSP_SED_Paper/"/>
    <id>http://yunniya097.github.io/2023/07/26/DSP_SED_Paper/</id>
    <published>2023-07-26T05:58:13.000Z</published>
    <updated>2023-07-31T06:26:47.080Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SOUND-EVENTS-INT-OUR-EVERYDAY-ENVIRONMENT"><a href="#SOUND-EVENTS-INT-OUR-EVERYDAY-ENVIRONMENT" class="headerlink" title="SOUND EVENTS INT OUR EVERYDAY ENVIRONMENT"></a>SOUND EVENTS INT OUR EVERYDAY ENVIRONMENT</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;SOUND-EVENTS-INT-OUR-EVERYDAY-ENVIRONMENT&quot;&gt;&lt;a href=&quot;#SOUND-EVENTS-INT-OUR-EVERYDAY-ENVIRONMENT&quot; class=&quot;headerlink&quot; title=&quot;SOUND EVEN</summary>
      
    
    
    
    <category term="Paper Review" scheme="http://yunniya097.github.io/categories/Paper-Review/"/>
    
    <category term="DSP" scheme="http://yunniya097.github.io/categories/Paper-Review/DSP/"/>
    
    
    <category term="Paper Review" scheme="http://yunniya097.github.io/tags/Paper-Review/"/>
    
    <category term="SED" scheme="http://yunniya097.github.io/tags/SED/"/>
    
    <category term="DSP" scheme="http://yunniya097.github.io/tags/DSP/"/>
    
  </entry>
  
  <entry>
    <title>Expectation</title>
    <link href="http://yunniya097.github.io/2023/07/23/Statistics_Expectation/"/>
    <id>http://yunniya097.github.io/2023/07/23/Statistics_Expectation/</id>
    <published>2023-07-23T12:58:13.000Z</published>
    <updated>2023-07-26T05:54:35.506Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Expectation-of-a-Random-Variable"><a href="#Expectation-of-a-Random-Variable" class="headerlink" title="Expectation of a Random Variable"></a>Expectation of a Random Variable</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Expectation-of-a-Random-Variable&quot;&gt;&lt;a href=&quot;#Expectation-of-a-Random-Variable&quot; class=&quot;headerlink&quot; title=&quot;Expectation of a Random Vari</summary>
      
    
    
    
    
    <category term="statistics" scheme="http://yunniya097.github.io/tags/statistics/"/>
    
  </entry>
  
</feed>
