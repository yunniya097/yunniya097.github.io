<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>YUNNI-CODING</title>
  
  <subtitle>Tech-Blog</subtitle>
  <link href="http://yunniya097.github.io/atom.xml" rel="self"/>
  
  <link href="http://yunniya097.github.io/"/>
  <updated>2023-08-21T06:44:27.598Z</updated>
  <id>http://yunniya097.github.io/</id>
  
  <author>
    <name>Yunniya</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Signal과 Digital Signal Processing</title>
    <link href="http://yunniya097.github.io/2023/08/21/DSP_Concept_1/"/>
    <id>http://yunniya097.github.io/2023/08/21/DSP_Concept_1/</id>
    <published>2023-08-21T06:14:56.000Z</published>
    <updated>2023-08-21T06:44:27.598Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Signal"><a href="#Signal" class="headerlink" title="Signal"></a><strong>Signal</strong></h2><p>신호(signal)란 무엇일까? 신호는 여러가지 정의를 가지고 있지만 전자공학에서의 정의는 시간에 따라 변하는 양이다. 전자공학에서 신호는 크게 아날로그 신호와 디지털 신호로 구분된다.</p><h3 id="Analog-Signal"><a href="#Analog-Signal" class="headerlink" title="Analog Signal"></a><strong>Analog Signal</strong></h3><p>시간에 따라 연속적(continous)으로 변화하는 신호를 말하며, 자연 현상에서 발생하는 신호에 가깝다고 볼 수 있다.</p><h3 id="Digital-Signal"><a href="#Digital-Signal" class="headerlink" title="Digital Signal"></a><strong>Digital Signal</strong></h3><p>이산적(descrete)으로 정보를 표현하여 메모리에 저장이 가능하게 한 신호를 말하며, 0과 1로 표현 가능하다.</p><h2 id="Digital-Signal-Processing"><a href="#Digital-Signal-Processing" class="headerlink" title="Digital Signal Processing"></a><strong>Digital Signal Processing</strong></h2><p>analog signal을 digital signal로 바꾸기 위해서는 다음과 같은 과정을 거쳐야 한다.</p><h3 id="Sampling"><a href="#Sampling" class="headerlink" title="Sampling"></a><strong>Sampling</strong></h3><p>sampling은 원본에서 sample만 추출하겠다는 의미로, 연속적인 아날로그 신호를 어떤 기준에 의해 잘게 나눠 discrete한 디지털 신호로 표현하는 과정을 말한다. 이때 얼마나 촘촘하게 signal을 뽑아내느냐(&#x3D;sampling Rate을 어떻게 설정하느냐)에 따라 성능이 달라질 수 있기 때문에 어떻게 sampling 하는지가 중요하다. sampling에 사용되는 용어 정의는 다음과 같다.</p><ul><li><strong>Sample</strong>: 특정 시점에서의 signal 값</li><li><strong>Sampling Period</strong>: sampling을 얼마만큼의 시간마다 할 것인가 (sec)</li><li><strong>Sampling Rate</strong>: 1초동안 sampling을 몇번 했는가 (Hz)</li></ul><p>위에서 sampling rate를 어떻게 설정하냐에 따라 성능이 달라질 수 있다고 이야기 했는데 그 이유는 다음과 같다. 먼저 sampling rate를 높게 설정하게 되면 그만큼 신호를 더 잘게 쪼개게 되는데 이렇게 되면 그만큼 sample 수도 늘어나기 때문에 메모리를 많이 찾이하게 된다는 단점이 있다.</p><p>반대로 sampling rate을 낮게 설정하면 analog signal의 정보를 많이 뽑아내지 못하기 때문에 analog signal과 digital signal 사이에 오차가 커지게 되어 aliasing이라는 문제가 발생한다.</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Signal&quot;&gt;&lt;a href=&quot;#Signal&quot; class=&quot;headerlink&quot; title=&quot;Signal&quot;&gt;&lt;/a&gt;&lt;strong&gt;Signal&lt;/strong&gt;&lt;/h2&gt;&lt;p&gt;신호(signal)란 무엇일까? 신호는 여러가지 정의를 가지고 있지</summary>
      
    
    
    
    <category term="DSP" scheme="http://yunniya097.github.io/categories/DSP/"/>
    
    <category term="Concept" scheme="http://yunniya097.github.io/categories/DSP/Concept/"/>
    
    
    <category term="DSP" scheme="http://yunniya097.github.io/tags/DSP/"/>
    
  </entry>
  
  <entry>
    <title>BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension 정리</title>
    <link href="http://yunniya097.github.io/2023/07/31/NLP_BART_Paper/"/>
    <id>http://yunniya097.github.io/2023/07/31/NLP_BART_Paper/</id>
    <published>2023-07-31T06:26:03.000Z</published>
    <updated>2023-08-03T08:40:16.400Z</updated>
    
    <content type="html"><![CDATA[<p>📍 Facebook에서 발표한 논문<br>📍 ACL 2020 accepted</p><p>📄<a href="https://arxiv.org/pdf/1910.13461.pdf">논문 원본</a></p><h2 id="ABSTRACT"><a href="#ABSTRACT" class="headerlink" title="ABSTRACT"></a><strong>ABSTRACT</strong></h2><p>💡 본 논문에서 제안하는 내용</p><ul><li>pre-train을 위한 Seq2Seq model, BART 제안</li><li>BART → standard Transformer-based  Architecture<ul><li>corrupted text를 original text에 mapping denoising autoencoder</li></ul></li></ul><hr><h2 id="INTRODUCTION"><a href="#INTRODUCTION" class="headerlink" title="INTRODUCTION"></a><strong>INTRODUCTION</strong></h2><p>💡 <strong>이전 연구</strong></p><p>self-supervised method는 다양한 NLP task에서 주목할만한 성능을 보여줌</p><ul><li><p>그중 단어의 subset이 random masked text를 reconstruct 하도록 학습된 denoising autoencoder(DAE) 방법이 성공적 → MLM의 변형</p></li><li><p>maksed token의 distribution, 예측 순서, 대체 가능한 context 등을 개선해 더 나은 결과를 보여줌</p></li><li><p>self-supervised method 정의</p><p>  : unlabeled data로 좋은 representation을 얻고자하는 학습 방식</p><p>  → <strong>label(y) 없이 input(x) 내에서 target으로 쓰일만 한 것을 정해서 즉 self로 task를 정해서 supervision 방식으로  모델을 학습</strong></p></li></ul><details><summary>DAE</summary><div markdown="1"><p>말그대로 input에 noise를 추가했음에도 디코더의 output은 noise를 추가하지 않은 원본을 복원하도록 하자는 아이디어</p></div></details>        <br/>💡 **제안 배경**<p>당시 최근 연구 방법들은 일반적으로 span prediction이나 generation과 같은 특정한 유형의 end task에 초점을 맞춤 → 적용 가능성이 제한적</p><p>ex) BERT는 generation task 적용 불가능, GPT는 bidirectional context 정보 반영 불가능</p><p>💡 <strong>BART</strong></p><p>: standard Transformer-based architecture를 가지며, <strong>B</strong>idirectional과 <strong>A</strong>uto-<strong>R</strong>egressive <strong>T</strong>ransformer를 합친 model</p><p>⇒ BERT의 encoder + GPT의 decoder</p><p><img src="/Users/seyunahn/yunniya.github.io/themes/icarus/source/img/post/NLP_BART_Paper/Untitled.png" alt="image"></p><blockquote><h2 id="a-BERT-random-token이-mask로-대체되고-문서가-bidirectionl로-encoding됨-누락된-token은-독립적으로-예측되므로-BERT는-generation에-쉽게-사용할-수-x"><a href="#a-BERT-random-token이-mask로-대체되고-문서가-bidirectionl로-encoding됨-누락된-token은-독립적으로-예측되므로-BERT는-generation에-쉽게-사용할-수-x" class="headerlink" title="(a) BERT: random token이 mask로 대체되고 문서가 bidirectionl로 encoding됨 .누락된 token은 독립적으로 예측되므로 BERT는 generation에 쉽게 사용할 수 x"></a>(a) BERT: random token이 mask로 대체되고 문서가 bidirectionl로 encoding됨 .누락된 token은 독립적으로 예측되므로 BERT는 generation에 쉽게 사용할 수 x</h2><h2 id="b-GPT-token은-auto-regressive하게-예측되므로-GPT를-generation에-사용-가능-But-단어는-왼쪽-context에만-조건이-적용되므로-bidirectional-상호-작용을-학습할-수-x"><a href="#b-GPT-token은-auto-regressive하게-예측되므로-GPT를-generation에-사용-가능-But-단어는-왼쪽-context에만-조건이-적용되므로-bidirectional-상호-작용을-학습할-수-x" class="headerlink" title="(b) GPT: token은 auto-regressive하게 예측되므로 GPT를 generation에 사용 가능. But 단어는 왼쪽 context에만 조건이 적용되므로 bidirectional 상호 작용을 학습할 수 x"></a>(b) GPT: token은 auto-regressive하게 예측되므로 GPT를 generation에 사용 가능. But 단어는 왼쪽 context에만 조건이 적용되므로 bidirectional 상호 작용을 학습할 수 x</h2><p>(c) BART: encoder에 대한 input이 decoder의 output과 일치할 필요가 없으므로 arbitary noise transformation이 가능. 여기서는 text의 span을 &lt;MASK&gt;로 대체하여 문서가 손상됨 → 손상된 문서(왼쪽)는 bidirectional model로 encoding한 다음 autoregressive decoder를 사용하여 원본 문서(오른쪽)의 가능성을 계산. fine-tuning을 위해 손상되지 않은 문서를 encoder와 decoder에 모두 입력한 후 decoder의 final hidden state의 represenation을 사용.</p></blockquote><p><strong>2-step of Pre-train</strong></p><ol><li><p>임의의 noising function으로 text를 손상시킴</p></li><li><p>original text를 reconstruct하도록 Seq2Seq를 학습</p></li></ol><p><strong>장점</strong></p><ul><li>noising flexibility → original text에 길이 변경을 포함한 임의의 transformation 적용 가능<ul><li><p>여러 noising approach 실험</p><p>  →  <strong>(1) original text의 순서를 랜덤으로 섞는 방식</strong> + <strong>(2) 임의의 길이 범위(0 포함)의 text를 single mask token으로 대체하는 새로운 in-filling 방식</strong></p><p>  → SOTA를 이루는 approach 발견</p></li><li><p>model이 전체 문장 길이에 대해 더 많이 추론하고, input에 더 긴 범위의 transformation을 수행하게 함 → MLM과 NSP를 generalize</p></li></ul></li><li>text generation을 위한 fine-tuning에 효과적, comprehension task에서도 잘 작동</li><li>fine-tuning에 대한 새로운 사고 방식 확장<ul><li><p>BART 위에 추가적인 transformer layer를 쌓음</p><p>  → 외국어를 noising된 영어로 mapping하고 이를 다시 denoising하는데 사용함으로써 BART가 강력한 번역모델로 동작하게 함</p></li></ul></li></ul><hr>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;📍 Facebook에서 발표한 논문&lt;br&gt;📍 ACL 2020 accepted&lt;/p&gt;
&lt;p&gt;📄&lt;a href=&quot;https://arxiv.org/pdf/1910.13461.pdf&quot;&gt;논문 원본&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&quot;ABSTRACT&quot;&gt;&lt;a h</summary>
      
    
    
    
    <category term="Paper Review" scheme="http://yunniya097.github.io/categories/Paper-Review/"/>
    
    <category term="NLP" scheme="http://yunniya097.github.io/categories/Paper-Review/NLP/"/>
    
    
    <category term="Paper Review" scheme="http://yunniya097.github.io/tags/Paper-Review/"/>
    
    <category term="PLM" scheme="http://yunniya097.github.io/tags/PLM/"/>
    
    <category term="NLP" scheme="http://yunniya097.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Sound Event Detection: A Tutorial 논문 정리</title>
    <link href="http://yunniya097.github.io/2023/07/26/DSP_SED_Paper/"/>
    <id>http://yunniya097.github.io/2023/07/26/DSP_SED_Paper/</id>
    <published>2023-07-26T05:58:13.000Z</published>
    <updated>2023-07-31T06:26:47.080Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SOUND-EVENTS-INT-OUR-EVERYDAY-ENVIRONMENT"><a href="#SOUND-EVENTS-INT-OUR-EVERYDAY-ENVIRONMENT" class="headerlink" title="SOUND EVENTS INT OUR EVERYDAY ENVIRONMENT"></a>SOUND EVENTS INT OUR EVERYDAY ENVIRONMENT</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;SOUND-EVENTS-INT-OUR-EVERYDAY-ENVIRONMENT&quot;&gt;&lt;a href=&quot;#SOUND-EVENTS-INT-OUR-EVERYDAY-ENVIRONMENT&quot; class=&quot;headerlink&quot; title=&quot;SOUND EVEN</summary>
      
    
    
    
    <category term="Paper Review" scheme="http://yunniya097.github.io/categories/Paper-Review/"/>
    
    <category term="DSP" scheme="http://yunniya097.github.io/categories/Paper-Review/DSP/"/>
    
    
    <category term="DSP" scheme="http://yunniya097.github.io/tags/DSP/"/>
    
    <category term="SED" scheme="http://yunniya097.github.io/tags/SED/"/>
    
    <category term="Paper Review" scheme="http://yunniya097.github.io/tags/Paper-Review/"/>
    
  </entry>
  
  <entry>
    <title>Expectation</title>
    <link href="http://yunniya097.github.io/2023/07/23/Statistics_Expectation/"/>
    <id>http://yunniya097.github.io/2023/07/23/Statistics_Expectation/</id>
    <published>2023-07-23T12:58:13.000Z</published>
    <updated>2023-07-26T05:54:35.506Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Expectation-of-a-Random-Variable"><a href="#Expectation-of-a-Random-Variable" class="headerlink" title="Expectation of a Random Variable"></a>Expectation of a Random Variable</h2>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Expectation-of-a-Random-Variable&quot;&gt;&lt;a href=&quot;#Expectation-of-a-Random-Variable&quot; class=&quot;headerlink&quot; title=&quot;Expectation of a Random Vari</summary>
      
    
    
    
    
    <category term="statistics" scheme="http://yunniya097.github.io/tags/statistics/"/>
    
  </entry>
  
</feed>
